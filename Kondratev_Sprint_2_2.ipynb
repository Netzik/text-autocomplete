{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "i90ueoIV0TFA"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Imports\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('text-autocomplete')\n",
    "import requests\n",
    "from src.data_utils import *\n",
    "from src.eval_transformer_pipeline import TransformerEvaluator\n",
    "from src.next_token_dataset import NextTokenDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from src.lstm_model import create_model\n",
    "from src.lstm_train import train\n",
    "from src.eval_lstm import evaluate_lstm, print_examples\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"dataset/tweets.txt\"\n",
    "URL = \"https://code.s3.yandex.net/deep-learning/tweets.txt\"\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(\"Downloading dataset...\")\n",
    "    os.makedirs(\"dataset\", exist_ok=True)\n",
    "    r = requests.get(URL)\n",
    "    with open(DATA_PATH, \"wb\") as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "unsKl58N1C1r"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "VOCAB_SIZE = 100_000\n",
    "BATCH_SIZE = 1024\n",
    "N_SAMPLES = 500_000  # use 500k tweets for faster experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gO7YS-_M1ImA",
    "outputId": "f454cfac-93de-468e-f300-e153e0520975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw texts...\n",
      "Loaded 500000 raw texts\n",
      "\n",
      "Cleaning texts...\n",
      "After cleaning: 495681 texts\n",
      "\n",
      "Splitting into train/val/test...\n",
      "Train: 396544, Val: 49568, Test: 49569\n",
      "\n",
      "Building vocabulary...\n",
      "Vocabulary size: 100000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load and preprocess data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading raw texts...\")\n",
    "raw_texts = load_texts(DATA_PATH, n=N_SAMPLES)\n",
    "print(f\"Loaded {len(raw_texts)} raw texts\")\n",
    "\n",
    "print(\"\\nCleaning texts...\")\n",
    "clean_texts = [clean(t) for t in raw_texts if len(clean(t).split()) > 1]\n",
    "print(f\"After cleaning: {len(clean_texts)} texts\")\n",
    "\n",
    "print(\"\\nSplitting into train/val/test...\")\n",
    "train_texts, val_texts, test_texts = split_dataset(clean_texts)\n",
    "print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
    "\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "stoi, itos = build_vocab(train_texts, VOCAB_SIZE)\n",
    "print(f\"Vocabulary size: {len(stoi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEbX4t1q1O1t",
    "outputId": "6d2fb05a-da4c-43fa-d2ca-e8509b6f4f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Train dataset: 396544 examples\n",
      "Val dataset: 49568 examples\n",
      "\n",
      "Creating dataloaders...\n",
      "\n",
      "Batch shapes: x=torch.Size([1024, 31]), y=torch.Size([1024, 31])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Create datasets and dataloaders\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_ds = NextTokenDataset(train_texts, stoi)\n",
    "val_ds = NextTokenDataset(val_texts, stoi)\n",
    "print(f\"Train dataset: {len(train_ds)} examples\")\n",
    "print(f\"Val dataset: {len(val_ds)} examples\")\n",
    "\n",
    "print(\"\\nCreating dataloaders...\")\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: NextTokenDataset.collate_fn(b, stoi[PAD])\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda b: NextTokenDataset.collate_fn(b, stoi[PAD])\n",
    ")\n",
    "\n",
    "# check batch shape\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes: x={x.shape}, y={y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opuTZOX50b4N",
    "outputId": "226acac7-2eed-409b-c4cb-a933bed1c708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RAW TWEETS (first 10)\n",
      "======================================================================\n",
      "0: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third D\n",
      "1: is upset that he can't update his Facebook by texting it... and might cry as a result  School today \n",
      "2: @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "3: my whole body feels itchy and like its on fire\n",
      "4: @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all o\n",
      "5: @Kwesidei not the whole crew\n",
      "6: Need a hug\n",
      "7: @LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
      "8: @Tatiana_K nope they didn't have it\n",
      "9: @twittera que me muera ?\n",
      "\n",
      "======================================================================\n",
      "CLEANED TWEETS (first 10)\n",
      "======================================================================\n",
      "0: a thats a bummer you shoulda got david carr of third day to do it d\n",
      "1: is upset that he cant update his facebook by texting it and might cry as a result school today also \n",
      "2: i dived many times for the ball managed to save the rest go out of bounds\n",
      "3: my whole body feels itchy and like its on fire\n",
      "4: no its not behaving at all im mad why am i here because i cant see you all over there\n",
      "5: not the whole crew\n",
      "6: need a hug\n",
      "7: hey long time no see yes rains a bit only a bit lol im fine thanks hows you\n",
      "8: nope they didnt have it\n",
      "9: que me muera\n",
      "\n",
      "======================================================================\n",
      "TOP 20 WORDS IN VOCABULARY\n",
      "======================================================================\n",
      " 1. 'i' -> 3\n",
      " 2. 'to' -> 4\n",
      " 3. 'the' -> 5\n",
      " 4. 'my' -> 6\n",
      " 5. 'a' -> 7\n",
      " 6. 'and' -> 8\n",
      " 7. 'is' -> 9\n",
      " 8. 'it' -> 10\n",
      " 9. 'in' -> 11\n",
      "10. 'im' -> 12\n",
      "11. 'for' -> 13\n",
      "12. 'of' -> 14\n",
      "13. 'you' -> 15\n",
      "14. 'me' -> 16\n",
      "15. 'so' -> 17\n",
      "16. 'on' -> 18\n",
      "17. 'have' -> 19\n",
      "18. 'but' -> 20\n",
      "19. 'not' -> 21\n",
      "20. 'that' -> 22\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE FROM BATCH\n",
      "======================================================================\n",
      "Input sequence:  its kind of cold out today\n",
      "Target sequence: kind of cold out today\n",
      "(notice target is shifted by 1 token)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Inspect data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RAW TWEETS (first 10)\")\n",
    "print(\"=\"*70)\n",
    "for i, text in enumerate(raw_texts[:10]):\n",
    "    print(f\"{i}: {text[:100]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLEANED TWEETS (first 10)\")\n",
    "print(\"=\"*70)\n",
    "for i, text in enumerate(clean_texts[:10]):\n",
    "    print(f\"{i}: {text[:100]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 20 WORDS IN VOCABULARY\")\n",
    "print(\"=\"*70)\n",
    "vocab_words = [w for w in stoi.keys() if w not in [PAD, UNK, EOS]][:20]\n",
    "for i, word in enumerate(vocab_words):\n",
    "    print(f\"{i+1:2d}. '{word}' -> {stoi[word]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE FROM BATCH\")\n",
    "print(\"=\"*70)\n",
    "x_sample = x[0]\n",
    "y_sample = y[0]\n",
    "\n",
    "# find actual length (excluding padding)\n",
    "seq_len = (x_sample != stoi[PAD]).sum().item()\n",
    "\n",
    "# decode tokens\n",
    "x_words = [itos[int(idx)] for idx in x_sample[:seq_len] if int(idx) not in [stoi[PAD], stoi[EOS]]]\n",
    "y_words = [itos[int(idx)] for idx in y_sample[:seq_len] if int(idx) not in [stoi[PAD], stoi[EOS]]]\n",
    "\n",
    "print(f\"Input sequence:  {' '.join(x_words)}\")\n",
    "print(f\"Target sequence: {' '.join(y_words)}\")\n",
    "print(f\"(notice target is shifted by 1 token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSpL0Zn257_O",
    "outputId": "b10235d7-cbca-4be5-fc12-9312f854947c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LSTM model with 39,421,600 parameters\n",
      "Forward pass test: input torch.Size([1024, 33]) -> logits torch.Size([1024, 33, 100000])\n",
      "\n",
      "Test generation:\n",
      "Prefix: 'i am'\n",
      "Generated: 'i am receptionist receptionist empretty methquot methquot chruch visits manbits prieta ulumnya'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Test LSTM Model\n",
    "# ============================================================================\n",
    "\n",
    "model = create_model(\n",
    "    vocab_size=len(stoi),\n",
    "    embed_size=128,\n",
    "    hidden_size=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# test forward pass\n",
    "x_test, y_test = next(iter(train_loader))\n",
    "logits = model(x_test)\n",
    "print(f\"Forward pass test: input {x_test.shape} -> logits {logits.shape}\")\n",
    "\n",
    "# test greedy generation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "test_prefix = \"i am\"\n",
    "test_tokens = tokenize(test_prefix, stoi)\n",
    "print(f\"\\nTest generation:\")\n",
    "print(f\"Prefix: '{test_prefix}'\")\n",
    "\n",
    "generated = model.generate_greedy(\n",
    "    start_tokens=test_tokens,\n",
    "    max_new_tokens=10,\n",
    "    eos_idx=stoi[EOS],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "generated_text = ' '.join([itos[t] for t in generated])\n",
    "print(f\"Generated: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7KXnSbGH81zc",
    "outputId": "7c50609a-321b-492c-fddf-ed808876fab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "\n",
      "======================================================================\n",
      "TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:35<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | Loss: 6.9267 | Val ROUGE-1: 0.3667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:30<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/10 | Loss: 6.4276 | Val ROUGE-1: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:30<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/10 | Loss: 6.0629 | Val ROUGE-1: 0.3787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:30<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/10 | Loss: 5.8473 | Val ROUGE-1: 0.3633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:31<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/10 | Loss: 5.7049 | Val ROUGE-1: 0.3917\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Examples after epoch 5:\n",
      "----------------------------------------------------------------------\n",
      "Prefix:    see thats\n",
      "Target:    why\n",
      "Greedy:    a\n",
      "Sampled:   making\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:30<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/10 | Loss: 5.5916 | Val ROUGE-1: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/10 | Loss: 5.4902 | Val ROUGE-1: 0.3833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:31<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/10 | Loss: 5.3882 | Val ROUGE-1: 0.3800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:31<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/10 | Loss: 5.2883 | Val ROUGE-1: 0.3917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 388/388 [02:30<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 5.1973 | Val ROUGE-1: 0.3444\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Examples after epoch 10:\n",
      "----------------------------------------------------------------------\n",
      "Prefix:    see thats\n",
      "Target:    why\n",
      "Greedy:    a\n",
      "Sampled:   when\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Train LSTM Model\n",
    "# ============================================================================\n",
    "\n",
    "# training config\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# train model\n",
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    eos_idx=stoi[EOS],\n",
    "    epochs=EPOCHS,\n",
    "    lr=LR,\n",
    "    device=device,\n",
    "    print_examples_every=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Au3DFvSpT17O",
    "outputId": "a3c1392c-dcb3-4d6a-8bee-109be7783347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: text-autocomplete/models/lstm_model.pt\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Save Model\n",
    "# ============================================================================\n",
    "\n",
    "os.makedirs('text-autocomplete/models', exist_ok=True)\n",
    "\n",
    "model_path = 'text-autocomplete/models/lstm_model.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': len(stoi),\n",
    "    'embed_size': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.1,\n",
    "    'stoi': stoi,\n",
    "    'itos': itos,\n",
    "    'config': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'lr': LR\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "zgwmiltuU0pn",
    "outputId": "64a86ef1-b72e-4a8f-f1b0-6e3c61651dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set (GREEDY decoding)...\n",
      "\n",
      "======================================================================\n",
      "VALIDATION ROUGE SCORES (GREEDY)\n",
      "======================================================================\n",
      "rouge1      : 0.1166\n",
      "rouge2      : 0.0215\n",
      "rougeL      : 0.1163\n",
      "rougeLsum   : 0.1163\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GENERATION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Prefix:    not really its gone too far for me to even get whats going on that only makes it\n",
      "Target:    even scarier so i dont watch it\n",
      "Greedy:    worse\n",
      "Sampled:   go too the day on the own\n",
      "\n",
      "Example 2:\n",
      "Prefix:    cant say i have that <unk>\n",
      "Target:    the <unk> <unk>\n",
      "Greedy:    \n",
      "Sampled:   thingtoo\n",
      "\n",
      "Example 3:\n",
      "Prefix:    i wish you could go to the\n",
      "Target:    dentist for me\n",
      "Greedy:    beach\n",
      "Sampled:   wedding show i\n",
      "\n",
      "Example 4:\n",
      "Prefix:    oh sooooo bored broke a string\n",
      "Target:    practicing fml\n",
      "Greedy:    of the\n",
      "Sampled:   cousin will\n",
      "\n",
      "Example 5:\n",
      "Prefix:    i missed yall y did yall\n",
      "Target:    go first smh\n",
      "Greedy:    get to see\n",
      "Sampled:   do that bit\n",
      "\n",
      "Example 6:\n",
      "Prefix:    why do some people hate me ox it\n",
      "Target:    makes me sad\n",
      "Greedy:    is so sad\n",
      "Sampled:   sucks i want\n",
      "\n",
      "Example 7:\n",
      "Prefix:    good morning followers its not a nice day today the rain is\n",
      "Target:    out and its cold\n",
      "Greedy:    so bad\n",
      "Sampled:   going to be a\n",
      "\n",
      "Example 8:\n",
      "Prefix:    is bored i feel like dancing but cant baby\n",
      "Target:    sitting b bcc\n",
      "Greedy:    \n",
      "Sampled:   just is heading\n",
      "\n",
      "Example 9:\n",
      "Prefix:    its official were moving to murrieta in days\n",
      "Target:    this freaken sucks\n",
      "Greedy:    \n",
      "Sampled:   is the past\n",
      "\n",
      "Example 10:\n",
      "Prefix:    eating in front the computer i cant\n",
      "Target:    do it hehe\n",
      "Greedy:    sleep\n",
      "Sampled:   find the tip\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Evaluate LSTM Model \n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nEvaluating on validation set (GREEDY decoding)...\")\n",
    "val_scores = evaluate_lstm(\n",
    "    model=model,\n",
    "    data_loader=val_loader,\n",
    "    itos=itos,\n",
    "    eos_idx=stoi[EOS],\n",
    "    pad_idx=stoi[PAD],\n",
    "    device=device,\n",
    "    num_samples=None,\n",
    "    use_greedy=True  \n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION ROUGE SCORES (GREEDY)\")\n",
    "print(\"=\"*70)\n",
    "for metric, score in val_scores.items():\n",
    "    print(f\"{metric:12s}: {score:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print_examples(\n",
    "    model=model,\n",
    "    data_loader=val_loader,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    eos_idx=stoi[EOS],\n",
    "    device=device,\n",
    "    num_examples=10,\n",
    "    show_both=True  # –ø–æ–∫–∞–∂–µ—Ç –∏ greedy, –∏ sampled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilgpt2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fc74117bda442fbc9086718b3a14d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Model has 81,912,576 parameters\n",
      "\n",
      "Evaluating DistilGPT2...\n",
      "\n",
      "Evaluating on 49568 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49568/49568 [1:22:35<00:00, 10.00it/s]\n",
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "DISTILGPT2 ROUGE SCORES\n",
      "======================================================================\n",
      "rouge1      : 0.0339\n",
      "rouge2      : 0.0019\n",
      "rougeL      : 0.0326\n",
      "rougeLsum   : 0.0328\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "GENERATION EXAMPLES (DistilGPT2)\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Prefix:    not really its gone too far for me to even get whats going on that only makes it eve\n",
      "Target:    n scarier so i dont watch it\n",
      "Greedy:    .\n",
      "Sampled:   . Just about anything that is not going to make it go well.\n",
      "\n",
      "Example 2:\n",
      "Prefix:    cant say i have that probyr the onl\n",
      "Target:    ywho talksme\n",
      "Greedy:    ays are not going to be able to get the money.\n",
      "Sampled:   anes and my t-shirts are still going to be doing that.\n",
      "If you are interested in\n",
      "\n",
      "Example 3:\n",
      "Prefix:    i wish you could go to the den\n",
      "Target:    tist for me\n",
      "Greedy:    of the city of D√ºsseldorf.\n",
      "Sampled:   of the house where he was tortured and killed, and then there is the house where you are tortured\n",
      "\n",
      "Example 4:\n",
      "Prefix:    oh sooooo bored broke a string pr\n",
      "Target:    acticing fml\n",
      "Greedy:    ong.\n",
      "I was so excited to see how much I could do. I was so excited to\n",
      "Sampled:   ong on the door of my apartment in the morning. The door was open and a man in the\n",
      "\n",
      "Example 5:\n",
      "Prefix:    i missed yall y did yall go\n",
      "Target:    first smh\n",
      "Greedy:    to the beach.\n",
      "Sampled:   yall done yall done yall done yall done yall done yall done yall\n",
      "\n",
      "Example 6:\n",
      "Prefix:    why do some people hate me ox it\n",
      "Target:    makes me sad\n",
      "Greedy:    's not my fault I'm not a racist, it's not my fault I'm not a racist\n",
      "Sampled:   is because I am a little boy, so this is the only way to be a boy. But\n",
      "\n",
      "Example 7:\n",
      "Prefix:    good morning followers its not a nice day today the rain\n",
      "Target:    is out and its cold\n",
      "Greedy:    is coming.\n",
      "Sampled:   was going to change the situation. It was just a matter of time before the rain was going to\n",
      "\n",
      "Example 8:\n",
      "Prefix:    is bored i feel like dancing but cant baby\n",
      "Target:    sitting b bcc\n",
      "Greedy:    i feel like dancing but cant baby i feel like dancing but cant baby i feel like dancing but cant\n",
      "Sampled:   i remember how i felt at first i still feel like dancing but cant baby i remember how i felt\n",
      "\n",
      "Example 9:\n",
      "Prefix:    its official were moving to murrieta in days th\n",
      "Target:    is freaken sucks\n",
      "Greedy:    awing the city‚Äôs streets.\n",
      "Sampled:   awing, with the government's top aide saying she had been informed she could not run.\n",
      "\n",
      "Example 10:\n",
      "Prefix:    eating in front the computer i can\n",
      "Target:    t do it hehe\n",
      "Greedy:    see the difference between the two.\n",
      "\n",
      "\n",
      "The first thing I noticed was that the first thing\n",
      "Sampled:   play in the background, so I just have to keep playing while they continue to play, so I\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Evaluate DistilGPT2\n",
    "# ============================================================================\n",
    "\n",
    "evaluator = TransformerEvaluator(model_name='distilgpt2', device=device)\n",
    "\n",
    "# evaluate on validation set\n",
    "print(\"\\nEvaluating DistilGPT2...\")\n",
    "gpt_scores = evaluator.evaluate(\n",
    "    texts=val_texts,\n",
    "    prefix_ratio=0.75,\n",
    "    num_samples=None,  \n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISTILGPT2 ROUGE SCORES\")\n",
    "print(\"=\"*70)\n",
    "for metric, score in gpt_scores.items():\n",
    "    print(f\"{metric:12s}: {score:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# print examples\n",
    "evaluator.print_examples(\n",
    "    texts=val_texts,\n",
    "    num_examples=10,\n",
    "    prefix_ratio=0.75,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL COMPARISON\n",
      "======================================================================\n",
      "                LSTM  DistilGPT2\n",
      "ROUGE-1     0.116622    0.033932\n",
      "ROUGE-2     0.021478    0.001943\n",
      "ROUGE-L     0.116340    0.032631\n",
      "ROUGE-Lsum  0.116340    0.032773\n",
      "======================================================================\n",
      "\n",
      "DistilGPT2 improvement over LSTM:\n",
      "ROUGE-1     : -70.9%\n",
      "ROUGE-2     : -91.0%\n",
      "ROUGE-L     : -72.0%\n",
      "ROUGE-Lsum  : -71.8%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Compare LSTM vs DistilGPT2\n",
    "# ============================================================================\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'LSTM': [\n",
    "        val_scores['rouge1'],\n",
    "        val_scores['rouge2'],\n",
    "        val_scores['rougeL'],\n",
    "        val_scores['rougeLsum']\n",
    "    ],\n",
    "    'DistilGPT2': [\n",
    "        gpt_scores['rouge1'],\n",
    "        gpt_scores['rouge2'],\n",
    "        gpt_scores['rougeL'],\n",
    "        gpt_scores['rougeLsum']\n",
    "    ]\n",
    "}, index=['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-Lsum'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison)\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDistilGPT2 improvement over LSTM:\")\n",
    "for metric in comparison.index:\n",
    "    lstm_score = comparison.loc[metric, 'LSTM']\n",
    "    gpt_score = comparison.loc[metric, 'DistilGPT2']\n",
    "    improvement = ((gpt_score - lstm_score) / lstm_score * 100) if lstm_score > 0 else 0\n",
    "    print(f\"{metric:12s}: {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "–§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï\n",
      "======================================================================\n",
      "\n",
      "Test dataset: 49569 examples\n",
      "\n",
      "Evaluating LSTM on TEST set (GREEDY decoding)...\n",
      "\n",
      "======================================================================\n",
      "TEST SET ROUGE SCORES (LSTM)\n",
      "======================================================================\n",
      "rouge1      : 0.1176\n",
      "rouge2      : 0.0210\n",
      "rougeL      : 0.1173\n",
      "rougeLsum   : 0.1173\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "VALIDATION vs TEST COMPARISON\n",
      "======================================================================\n",
      "–ú–µ—Ç—Ä–∏–∫–∞         Validation   Test         –†–∞–∑–Ω–∏—Ü–∞     \n",
      "----------------------------------------------------------------------\n",
      "rouge1          0.1166       0.1176       +0.0009\n",
      "rouge2          0.0215       0.0210       -0.0004\n",
      "rougeL          0.1163       0.1173       +0.0009\n",
      "rougeLsum       0.1163       0.1173       +0.0009\n",
      "======================================================================\n",
      "\n",
      "–û–¢–õ–ò–ß–ù–û: Test –∏ Validation scores –±–ª–∏–∑–∫–∏ - –Ω–µ—Ç overfitting\n",
      "\n",
      "======================================================================\n",
      "GENERATION EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "Example 1:\n",
      "Prefix:    grounded through the week not allowed to\n",
      "Target:    use the comp\n",
      "Greedy:    be in the\n",
      "Sampled:   go in a\n",
      "\n",
      "Example 2:\n",
      "Prefix:    just got home i think i broke\n",
      "Target:    my <unk> cords\n",
      "Greedy:    my <unk>\n",
      "Sampled:   getting ready to\n",
      "\n",
      "Example 3:\n",
      "Prefix:    oh dear i think its broken again ill try to fix it cant\n",
      "Target:    wait to replace this theme\n",
      "Greedy:    wait for the next year\n",
      "Sampled:   sleep\n",
      "\n",
      "Example 4:\n",
      "Prefix:    my favorite ld thread has been tainted i knew\n",
      "Target:    it couldnt last\n",
      "Greedy:    it was a\n",
      "Sampled:   no tissues today\n",
      "\n",
      "Example 5:\n",
      "Prefix:    stuck in <unk>\n",
      "Target:    traffic\n",
      "Greedy:    \n",
      "Sampled:   at\n",
      "\n",
      "Example 6:\n",
      "Prefix:    im not sure i didnt even know they had candy im guessing it woulda went on\n",
      "Target:    clearance or something but idk sorry\n",
      "Greedy:    the way to the beach\n",
      "Sampled:   the hourglass\n",
      "\n",
      "Example 7:\n",
      "Prefix:    its a gloomy dayand i am\n",
      "Target:    at work\n",
      "Greedy:    not going\n",
      "Sampled:   gonna miss\n",
      "\n",
      "Example 8:\n",
      "Prefix:    my ears bleeding\n",
      "Target:    again\n",
      "Greedy:    \n",
      "Sampled:   after\n",
      "\n",
      "Example 9:\n",
      "Prefix:    youtube giving me issues with the\n",
      "Target:    <unk> interview\n",
      "Greedy:    <unk>\n",
      "Sampled:   ipod up\n",
      "\n",
      "Example 10:\n",
      "Prefix:    omg i started reading the same book but for no reason i stopped but i\n",
      "Target:    got plans of reading it again\n",
      "Greedy:    cant\n",
      "Sampled:   have to get up\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Final Evaluation on Test Set\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"–§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_ds = NextTokenDataset(test_texts, stoi)\n",
    "test_loader = DataLoader(\n",
    "    test_ds, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda b: NextTokenDataset.collate_fn(b, stoi[PAD])\n",
    ")\n",
    "\n",
    "print(f\"\\nTest dataset: {len(test_ds)} examples\")\n",
    "\n",
    "# Evaluate \n",
    "print(\"\\nEvaluating LSTM on TEST set (GREEDY decoding)...\")\n",
    "test_scores = evaluate_lstm(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    itos=itos,\n",
    "    eos_idx=stoi[EOS],\n",
    "    pad_idx=stoi[PAD],\n",
    "    device=device,\n",
    "    num_samples=None,  \n",
    "    use_greedy=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST SET ROUGE SCORES (LSTM)\")\n",
    "print(\"=\"*70)\n",
    "for metric, score in test_scores.items():\n",
    "    print(f\"{metric:12s}: {score:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Val vs Test\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION vs TEST COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'–ú–µ—Ç—Ä–∏–∫–∞':<15} {'Validation':<12} {'Test':<12} {'–†–∞–∑–Ω–∏—Ü–∞':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for metric in ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']:\n",
    "    val_score = val_scores[metric]\n",
    "    test_score = test_scores[metric]\n",
    "    diff = test_score - val_score\n",
    "    print(f\"{metric:<15} {val_score:<12.4f} {test_score:<12.4f} {diff:+.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# check overfitting\n",
    "if test_scores['rouge1'] < val_scores['rouge1'] - 0.02:\n",
    "    print(\"\\n–í–ù–ò–ú–ê–ù–ò–ï: Test score —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∏–∂–µ Validation - –≤–æ–∑–º–æ–∂–µ–Ω overfitting\")\n",
    "elif test_scores['rouge1'] > val_scores['rouge1'] + 0.02:\n",
    "    print(\"\\n–•–û–†–û–®–û: Test score –≤—ã—à–µ Validation - –º–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ –≥–µ–Ω–µ—Ä–∞–ª–∏–∑—É–µ—Ç\")\n",
    "else:\n",
    "    print(\"\\n–û–¢–õ–ò–ß–ù–û: Test –∏ Validation scores –±–ª–∏–∑–∫–∏ - –Ω–µ—Ç overfitting\")\n",
    "\n",
    "print_examples(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    stoi=stoi,\n",
    "    itos=itos,\n",
    "    eos_idx=stoi[EOS],\n",
    "    device=device,\n",
    "    num_examples=10,\n",
    "    show_both=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "–ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢ –ü–û –ü–†–û–ï–ö–¢–£\n",
      "======================================================================\n",
      "\n",
      "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ LSTM –ú–û–î–ï–õ–ò:\n",
      "----------------------------------------------------------------------\n",
      "Dataset         ROUGE-1      ROUGE-2      ROUGE-L     \n",
      "----------------------------------------------------------------------\n",
      "Validation      0.1166       0.0215       0.1163      \n",
      "Test            0.1176       0.0210       0.1173      \n",
      "–†–∞–∑–Ω–∏—Ü–∞         0.0009       -0.0004      0.0009      \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìà –°–†–ê–í–ù–ï–ù–ò–ï LSTM vs DistilGPT2:\n",
      "----------------------------------------------------------------------\n",
      "–ú–æ–¥–µ–ª—å          ROUGE-1      ROUGE-2      –ü–∞—Ä–∞–º–µ—Ç—Ä—ã      \n",
      "----------------------------------------------------------------------\n",
      "LSTM            0.1176       0.0210         39,421,600\n",
      "DistilGPT2      0.0339       0.0019         81,912,576\n",
      "–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ    3.5         x 10.8        x          2.1x –º–µ–Ω—å—à–µ\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–Å–¢ –ü–û –ü–†–û–ï–ö–¢–£\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ LSTM –ú–û–î–ï–õ–ò:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Dataset':<15} {'ROUGE-1':<12} {'ROUGE-2':<12} {'ROUGE-L':<12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Validation':<15} {val_scores['rouge1']:<12.4f} {val_scores['rouge2']:<12.4f} {val_scores['rougeL']:<12.4f}\")\n",
    "print(f\"{'Test':<15} {test_scores['rouge1']:<12.4f} {test_scores['rouge2']:<12.4f} {test_scores['rougeL']:<12.4f}\")\n",
    "print(f\"{'–†–∞–∑–Ω–∏—Ü–∞':<15} {test_scores['rouge1']-val_scores['rouge1']:<12.4f} {test_scores['rouge2']-val_scores['rouge2']:<12.4f} {test_scores['rougeL']-val_scores['rougeL']:<12.4f}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nüìà –°–†–ê–í–ù–ï–ù–ò–ï LSTM vs DistilGPT2:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'–ú–æ–¥–µ–ª—å':<15} {'ROUGE-1':<12} {'ROUGE-2':<12} {'–ü–∞—Ä–∞–º–µ—Ç—Ä—ã':<15}\")\n",
    "print(\"-\" * 70)\n",
    "lstm_params = sum(p.numel() for p in model.parameters())\n",
    "gpt_params = sum(p.numel() for p in evaluator.model.parameters())\n",
    "print(f\"{'LSTM':<15} {test_scores['rouge1']:<12.4f} {test_scores['rouge2']:<12.4f} {lstm_params:>12,}\")\n",
    "print(f\"{'DistilGPT2':<15} {gpt_scores['rouge1']:<12.4f} {gpt_scores['rouge2']:<12.4f} {gpt_params:>12,}\")\n",
    "print(f\"{'–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ':<15} {test_scores['rouge1']/gpt_scores['rouge1']:<12.1f}x {test_scores['rouge2']/gpt_scores['rouge2']:<12.1f}x {gpt_params/lstm_params:>12.1f}x –º–µ–Ω—å—à–µ\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ò—Ç–æ–≥–æ–≤—ã–π –≤—ã–≤–æ–¥\n",
    "\n",
    "–î–ª—è –∑–∞–¥–∞—á–∏ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø—Ä–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º\n",
    "–æ–±—ä—ë–º–µ –¥–∞–Ω–Ω—ã—Ö –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —Ä–µ—à–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è\n",
    "—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è LSTM-–º–æ–¥–µ–ª—å.\n",
    "\n",
    "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –º–æ–≥—É—Ç –ø–æ–∫–∞–∑–∞—Ç—å –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–∏\n",
    "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–º –¥–æ–æ–±—É—á–µ–Ω–∏–∏ –∏–ª–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–º–∏\n",
    "–∏ —Å–ª–æ–∂–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏, –æ–¥–Ω–∞–∫–æ –≤ —Ç–µ–∫—É—â–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö LSTM\n",
    "–æ–∫–∞–∑–∞–ª–∞—Å—å –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π\n",
    "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "local/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
